{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ca4623",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Project 5: Railway Accidents\n",
    "Part 1: webscraping\n",
    "\n",
    "Objectives for this notebook:\n",
    "    \"1. Data Gathering: Crawl all Wikipedia articles on railway accidents,\n",
    "    use the corresponding page for 1815 as a starting point: \n",
    "    https://en.wikipedia.org/wiki/Category:Railway_accidents_in_1815\n",
    "    2. Find a strategy to exclude articles that do not directly refer to accidents.\n",
    "\n",
    "    3. Data Storage: Store the retrieved information in a CSV file. For each railway accident \n",
    "    store the following information in columns: \n",
    " - Date \n"
    " - location \n" 
    " - Coordinates (if available) \n" 
    " - Cause \n"
    " - Number of deaths\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6d88cf",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Setting the URL and start year for data collection\n",
    "URL = 'https://en.wikipedia.org/wiki/Category:Railway_accidents_in_'\n",
    "START_YEAR = 1815"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec96587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get a list of accident page URL´s for all years\n",
    "def get_accidents(soup):\n",
    "       # Finding all hyperlinks in the content area\n",
    "    # This class (HTML) represents the section of wikipedia that deals with all the articles.\n",
    "    # By finding all the \"href\" we find each URL for the articles.\n",
    "    links = soup.find(\"div\", {\"class\": \"mw-content-ltr\", 'dir':\"ltr\"}).find_all(href=True)\n",
    "    hrefs = [l.get('href') for l in links]\n",
    "    #Create empty list where we will save all the information\n",
    "    filtered = []\n",
    "    \n",
    "# In this loop we make sure to exclude all unwanted links    \n",
    "    for href in hrefs:\n",
    "        if href.startswith('/wiki/Category') or href.startswith('/wiki/Wikipedia:FAQ'):\n",
    "            pass\n",
    "        \n",
    "# Here we ensure to append the URLS we are interested in \n",
    "        elif href.startswith('/wiki/'):\n",
    "            filtered.append(href)\n",
    "            \n",
    "# If there are no articles found, we call the pages for \"no accidents\"    \n",
    "    if filtered[0] == '/wiki/Wikipedia:Categorization':\n",
    "        return \"No Accidents\"\n",
    "#We return the list of all the articles we are interested in\n",
    "    return filtered\n",
    "\n",
    "\n",
    "# This is a function to get the infobox data from a specific accident page.\n",
    "# (Almost) all train accident pages have an infobx with all the relevant information we need.\n",
    "def get_infobox(url):\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.content,'lxml')\n",
    "    \n",
    "# Here we create a simple filter by looking at the first paragraph of each article.\n",
    "# If it does not include these words, we expect it to not be a page about a train accident.    \n",
    "    num = 0\n",
    "    text = \"\"\n",
    "    for paragraph in soup.find_all('p', limit=2):\n",
    "        text += paragraph.text\n",
    "      \n",
    "    if text.find(\"train\") >= 0 or text.find(\"locomotive\") >= 0 or text.find(\"rail\") >= 0 or text.find(\"tram\") >= 0:\n",
    "        num += 1\n",
    "    if text.find(\"accident\") >= 0 or text.find(\"crash\") >= 0 or text.find(\"wreck\") >= 0 or text.find(\"derail\") >= 0 or text.find(\"colli\") >= 0 or text.find(\"disaster\") >= 0 or text.find(\"injur\") >= 0:\n",
    "        num += 1\n",
    "\n",
    "        \n",
    "# Here we find the infobox and get the information if there is an infobox. \n",
    "# If there are no infobox we will get the output \"no infobox\"        \n",
    "    if num == 2:\n",
    "        table = soup.select_one('.infobox.vcard')\n",
    "        title = soup.find(\"h1\", {\"id\": \"firstHeading\"}).get_text()\n",
    "      \n",
    "        try:\n",
    "            rows = table.find_all('tr')\n",
    "            output = {}\n",
    "            for row in rows:\n",
    "                if len(row.select('th, td')) == 2:\n",
    "                    key = row.select_one('th').text\n",
    "                    value = row.select_one('td').text\n",
    "                    output[key]=value\n",
    "                    output[\"Accident title\"] = title\n",
    "        except:\n",
    "            output = (\"No infobox\")\n",
    "    else:\n",
    "        output = (\"not a train accident\")\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3798b271",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#Here we make a list of all the URL´s and webscrape them from 1815 until 2021\n",
    "data = []\n",
    "YEAR = START_YEAR\n",
    "while YEAR <= 2021:\n",
    "    r = requests.get(URL + str(YEAR))\n",
    "    soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "    ACCIDENTS = get_accidents(soup)\n",
    "    data.append([YEAR, ACCIDENTS])\n",
    "    YEAR += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4d3483",
   "metadata": {
    "lines_to_next_cell": 3
   },
   "outputs": [],
   "source": [
    "# Storing the data in a Pandas DataFrame\n",
    "\n",
    "# Frist load data into dataframe\n",
    "df = pd.DataFrame(data, columns = ['year', 'accident'])\n",
    "\n",
    "# Then transforming each element of a list-like object to a row\n",
    "# By using this function it makes several rows, one for each accident within the same year. \n",
    "df = df.explode('accident')\n",
    "\n",
    "# Remove rows without accidents\n",
    "df = df[df.accident != 'No Accidents']\n",
    "\n",
    "# Add https://en.wikipedia.org/ to accident URL\n",
    "df['accident_url'] = 'https://en.wikipedia.org/' + df.accident\n",
    "\n",
    "# Reset index as we have removed the rows we labeled as \"no accident\"\n",
    "df = df.reset_index()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Adding progress bar for infobox extraction\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "# Applying the get_infobox function to each accident URL\n",
    "# Allows us to retrieve the relevant data from each article by calling the infobox function\n",
    "df['infobox'] = df.accident_url.progress_apply(get_infobox)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Extracting specific infobox data into separate columns\n",
    "df['Accident title'] = df.infobox.str['Accident title']\n",
    "df['Date'] = df.infobox.str['Date']\n",
    "df['Location'] = df.infobox.str['Location']\n",
    "df['Coordinates'] = df.infobox.str['Coordinates'].str.split(\"/\").str[-1]\n",
    "df['Cause'] = df.infobox.str['Cause']\n",
    "df['Deaths'] = df.infobox.str['Deaths']\n",
    "\n",
    "\n",
    "# Saving the DataFrame to a CSV file\n",
    "df.to_csv('WebscrapedTrainAccidents.csv', index=False, encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# -*- coding: utf-8 -*-",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
