{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb1fa38",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Project 5: Railway Accidents\n",
    "# Part 2: Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b9907c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Import pandas and re for cleaning the data\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# First we create a function we will need that splits the data up to extract numbers from string\n",
    "def numb(Strings):\n",
    "    newstr = ''.join((ch if ch in '0123456789' else ' ') for ch in i)\n",
    "    i_num = [float(j) for j in newstr.split()]\n",
    "    return i_num\n",
    "\n",
    "# Read the CSV file and create a DataFrame from the csv file we created in previous code\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/Julardzija/Webscraping-Train-Accidents-Wiki/main/Datasets/WebscrapedTrainAccidents.csv', encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea1948b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we want to remove the entries that is either not a train accident or has no infobox\n",
    "# This code makes sure that we remove all the rows that has these strings in infobox.\n",
    "# ~ sign means that it will find the opposite.\n",
    "df = df[~df.infobox.isin([\"No infobox\", \"not a train accident\"])]\n",
    "\n",
    "\n",
    "# Reset the index of the DataFrame and drop the old index\n",
    "# We do not neeed to remember the old index so we write True.\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "df = df.drop(columns=[\"index\"])\n",
    "df = df.drop(columns=[\"infobox\"])\n",
    "df = df.drop(columns=[\"accident\"])\n",
    "df = df.drop(columns=[\"accident_url\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2addbbff",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Process and clean the 'Accident title' column\n",
    "# We want to clean the accident titles as some of them have years included in them as\n",
    "# beginning of the title, which does not fit well.\n",
    "accident = []\n",
    "for i in df[\"Accident title\"]:\n",
    "    i = i.lower()\n",
    "    i = re.sub(r'[0-9]','', i)\n",
    "    accident.append(i)\n",
    "            \n",
    "\n",
    "\n",
    "df[\"Accident title\"] = accident\n",
    "#Removes any white space at the beginning of a string\n",
    "df[\"Accident title\"] = df[\"Accident title\"].str.lstrip()\n",
    "#We have some titles where the year is important part of the name. For these we put the\n",
    "#year of the accident back into the title.\n",
    "df.loc[df[\"Accident title\"].str.contains(\"crash of \"), \"Accident title\"] = df[\"Accident title\"] + df[\"year\"].apply(str)\n",
    "df.loc[df[\"Accident title\"].str.contains(\"wreck of \"), \"Accident title\"] = df[\"Accident title\"] + df[\"year\"].apply(str)\n",
    "#Make each first letter in the words big\n",
    "df['Accident title'] = df['Accident title'].str.title()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17dfa709",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Process and clean the 'Location' column\n",
    "# We have a lot of entries with footnotes (and normal brackets) inside of them. \n",
    "# We will make a code which we will reuse for many columns. But for now we will remove \n",
    "# square brakcet as well as normal brackets from Location.\n",
    "# Then we want to further clean it. First we make all letters lowercase.\n",
    "# We create a try except, to make sure the code can run throughout the data.\n",
    "df['Location'] = df['Location'].str.lower()\n",
    "location = []\n",
    "for i in df[\"Location\"]:\n",
    "    try:\n",
    "        i = re.sub(r\"\\[[^()]*\\]\", \"\", i)\n",
    "        i = re.sub(r\"\\([^()]*\\)\", \"\", i)\n",
    "        i = re.sub(r\"2.4 miles east of \", \"\", i)\n",
    "        i = re.sub(r\"3 km south of \", \"\", i)\n",
    "        i = re.sub(r\"9 miles  east of\", \"\", i)\n",
    "        i = re.sub('[0-9]', '', i)\n",
    "# This one removes a lot of coordinates. °.*$ means that whatever comes after the first symbol\n",
    "# needs to be removed until the end of the string. The symbol included in removing.\n",
    "        i = re.sub(r\"°.*$\", '', i)\n",
    "        i = re.sub(r\"coordinates.*$\", '', i)\n",
    "        i = re.sub(r\"km.*$\", '', i)\n",
    "        \n",
    "        #i = re.sub('km', '', i)\n",
    "        location.append(i)\n",
    "    except TypeError:\n",
    "        location.append(\"\")    \n",
    "            \n",
    "df[\"Location\"] = location\n",
    "# Now we want to split some of the data. There are many with too specific location followed by\n",
    "# \"near\" a specific city. We want this as this might be more useful when trying to find \n",
    "# coordinates for this particular place.\n",
    "\n",
    "# Split location values into two separate columns and clean up\n",
    "df[[\"loc1\", \"loc2\"]] = df[\"Location\"].str.split(\"near \", expand=True)\n",
    "\n",
    "df.loc[df.loc2.notnull(), \"loc1\"] = df.loc2\n",
    "df[\"Location\"] = df[\"loc1\"]\n",
    "df['Location'] = df['Location'].str.title()\n",
    "\n",
    "# Drop temporary columns\n",
    "df = df.drop(columns=[\"loc1\", \"loc2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e679fb9",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Process and clean the 'Coordinates' column\n",
    "# First we would want to separate coordinates into two separate columns latitude and longitude \n",
    "# which will help us for both cleaning & geomapping in the end. First we split between \";\"\n",
    "df[[\"Latitude\", \"Longitude\"]] = df[\"Coordinates\"].str.split(\";\", expand=True)\n",
    "\n",
    "# The cleaning issue is now only with the Longitude column which sometimes has additional info\n",
    "# This loop makes sure to clean it by removing any brackets and only giving us \n",
    "# numbers, dots and -   Similar code to location\n",
    "\n",
    "Long=[]\n",
    "for i in df[\"Longitude\"]:\n",
    "    try:\n",
    "        i = re.sub(r\"\\([^()]*\\)\", \"\",i)\n",
    "        i = re.sub(r\"\\[[^()]*\\]\", \"\",i)\n",
    "        i = re.sub('[^0-9, ., -]', ' ', i)\n",
    "        Long.append(i)\n",
    "            \n",
    "    except TypeError:\n",
    "        Long.append(\"\")\n",
    "\n",
    "df[\"Longitude\"] = Long\n",
    "\n",
    "\n",
    "# Now that Coordinates are clean, we will put them together and have a clean version together.\n",
    "df[\"Coordinates\"] = df[\"Latitude\"] + ';' + df[\"Longitude\"]\n",
    "\n",
    "# One of our tasks was to input the location if there was no coordinates in the data.\n",
    "# This code is conditional. It locates any entry in coordinates column. The insna() function\n",
    "# means we are looking for empty entries. If that is the case, then the row in coordinates\n",
    "# will be replaced with the corresponding entry for location in same row.\n",
    "df.loc[df.Coordinates.isna(), \"Coordinates\"] = df.Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370c5fd0",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Process and clean the 'Cause' column\n",
    "# For those that do not have any cause we want to write missing info\n",
    "df.loc[df.Cause.isna(), \"Cause\"] = \"Missing info\"\n",
    "\n",
    "# We need this column to behave similarly as we need to make a wordcloud. So we want it\n",
    "# to be similar. We first remove square & normal bracket, as well as ' \n",
    "# Then we make all words small\n",
    "# Lastly, we are interested in keeping three symbols: , ; and / \n",
    "# They separate different causes into the same entry, which we want to filtrate next.\n",
    "cause = []\n",
    "for i in df[\"Cause\"]:\n",
    "    try:\n",
    "        i = i.lower()\n",
    "        i = re.sub(r\"\\([^()]*\\)\", \"\",i)\n",
    "        i = re.sub(r\"\\[[^()]*\\]\", \"\",i)\n",
    "        i = re.sub('\\'', '', i)\n",
    "        i = re.sub('[^a-z, ;, \\,, /, :]', '', i)\n",
    "        cause.append(i)\n",
    "            \n",
    "    except AttributeError:\n",
    "        cause.append(\"\")\n",
    "\n",
    "df[\"Cause\"] = cause\n",
    "\n",
    "# Lastly we have a lot of different causes for accidents. They are separated in the string\n",
    "# with three signs seen below. We will split it and only keep the first cause string\n",
    "# This was a decision made so we easier could categorize each accident to only single cause type.\n",
    "df[\"Cause\"] = df[\"Cause\"].str.split(\";\", expand=True)[0]\n",
    "df[\"Cause\"] = df[\"Cause\"].str.split(\":\", expand=True)[0]\n",
    "df[\"Cause\"] = df[\"Cause\"].str.split(\"/\", expand=True)[0]\n",
    "df[\"Cause\"] = df[\"Cause\"].str.split(\"\\,\", expand=True)[0]\n",
    "\n",
    "\n",
    "# Lastly we want to create a new column with categories for cause types. This is to be able\n",
    "# to colorize them in the geomap when we visualize the final data.\n",
    "# Human Error\n",
    "df.loc[df[\"Cause\"].str.contains(\"human\"), \"Causetype\"] = \"Human Error\"\n",
    "df.loc[df[\"Cause\"].str.contains(\"driver\"), \"Causetype\"] = \"Human Error\"\n",
    "df.loc[df[\"Cause\"].str.contains(\"engineer\"), \"Causetype\"] = \"Human Error\"\n",
    "df.loc[df[\"Cause\"].str.contains(\"operator\"), \"Causetype\"] = \"Human Error\"\n",
    "df.loc[df[\"Cause\"].str.contains(\"load\"), \"Causetype\"] = \"Human Error\"\n",
    "df.loc[df[\"Cause\"].str.contains(\"weight\"), \"Causetype\"] = \"Human Error\"\n",
    "# Brake Failure/Error\n",
    "df.loc[df[\"Cause\"].str.contains(\"brak\"), \"Causetype\"] = \"Brake Failure/Error\"\n",
    "# Signalling Error\n",
    "df.loc[df[\"Cause\"].str.contains(\"signal\"), \"Causetype\"] = \"Signalling Error\"\n",
    "# Environmental Cause\n",
    "df.loc[df[\"Cause\"].str.contains(\"earthquake\"), \"Causetype\"] = \"Environmental Cause\"\n",
    "df.loc[df[\"Cause\"].str.contains(\"wind\"), \"Causetype\"] = \"Environmental Cause\"\n",
    "df.loc[df[\"Cause\"].str.contains(\"cloud\"), \"Causetype\"] = \"Environmental Cause\"\n",
    "df.loc[df[\"Cause\"].str.contains(\"landsl\"), \"Causetype\"] = \"Environmental Cause\"\n",
    "# Track/Rail Error\n",
    "df.loc[df[\"Cause\"].str.contains(\"rail\"), \"Causetype\"] = \"Track/Rail Error\"\n",
    "df.loc[df[\"Cause\"].str.contains(\"track\"), \"Causetype\"] = \"Track/Rail Error\"\n",
    "df.loc[df[\"Cause\"].str.contains(\"line\"), \"Causetype\"] = \"Track/Rail Error\"\n",
    "# Derailment\n",
    "df.loc[df[\"Cause\"].str.contains(\"derail\"), \"Causetype\"] = \"Derailment\"\n",
    "# Under Investigation\n",
    "df.loc[df[\"Cause\"].str.contains(\"investigat\"), \"Causetype\"] = \"Under Investigation\"\n",
    "# Technical Error\n",
    "df.loc[df[\"Cause\"].str.contains(\"axl\"), \"Causetype\"] = \"Technical Error\"\n",
    "df.loc[df[\"Cause\"].str.contains(\"wheel\"), \"Causetype\"] = \"Technical Error\"\n",
    "df.loc[df[\"Cause\"].str.contains(\"joint\"), \"Causetype\"] = \"Technical Error\"\n",
    "df.loc[df[\"Cause\"].str.contains(\"mainten\"), \"Causetype\"] = \"Technical Error\"\n",
    "df.loc[df[\"Cause\"].str.contains(\"coupling rod\"), \"Causetype\"] = \"Technical Error\"\n",
    "df.loc[df[\"Cause\"].str.contains(\"locomotive\"), \"Causetype\"] = \"Technical Error\"\n",
    "# Excessive Speed \n",
    "df.loc[df[\"Cause\"].str.contains(\"speed\"), \"Causetype\"] = \"Excessive Speed\"\n",
    "# Bridge Failure \n",
    "df.loc[df[\"Cause\"].str.contains(\"bridge\"), \"Causetype\"] = \"Bridge Failure\"\n",
    "# Cause information missing\n",
    "df.loc[df[\"Cause\"].str.contains(\"missing info\"), \"Causetype\"] = \"Cause information missing\"\n",
    "df.loc[df[\"Cause\"].str.contains(\"undetermin\"), \"Causetype\"] = \"Cause information missing\"\n",
    "# Other\n",
    "df.loc[df.Causetype.isna(), \"Causetype\"] = \"Other\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab721fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process and clean the 'Date' column\n",
    "#Creating an empty list for the dates, when they have been cleaned.\n",
    "Date_clean = []\n",
    "\n",
    "# Creating a list of all the months, so we can search for them.\n",
    "month = [\"January\", \"February\", \"March\", \"April\", \"May\", \"June\", \"July\", \"August\", \"September\", \"October\", \"November\", \"December\"]\n",
    "\n",
    "# Creating a for loop over all the date in the dataset.\n",
    "for i in range(len(df[\"Date\"])):\n",
    "\n",
    "# Saving the year for the accident.\n",
    "    year=df[\"year\"][i]\n",
    "    \n",
    "# Removing all the date, that is Not a Number (NaN or empty)    \n",
    "    if df[\"Date\"][i]==df[\"Date\"][i]:\n",
    "        \n",
    "# Making a string of the date.\n",
    "        i=df[\"Date\"][i]\n",
    "\n",
    "# Cleaning the string from everthing, that is not a letter or number.\n",
    "# Furthermore, we are removing all the [] and (), as well as what is written inside them.\n",
    "        i=re.sub(r\"\\([^()]*\\)\", \"\",i)\n",
    "        i=re.sub(r\"\\[[^()]*\\]\", \"\",i)\n",
    "        i=re.sub(\"\\xa0\", \" \",i)\n",
    "        i=re.sub('[^a-z,0-9,A-Z, ]', '', i)\n",
    "        \n",
    "# Splitting the string up into a list, with all the remaning elements.\n",
    "        i=i.split(\" \")\n",
    "        \n",
    "# Creating a list to save the month we find.\n",
    "        m=str()\n",
    "\n",
    "\n",
    "# Finding and saving the month from the string.\n",
    "        for j in month:\n",
    "            if j in i:\n",
    "                \n",
    "                m=j\n",
    "            \n",
    "# Since the date of a day is maximum of two numbers, then we are removing everything\n",
    "# that is longer the 2 elements.No other number than the date are left, since \n",
    "# all clocks and year are have 3 or more numbers.          \n",
    "        for j in range(len(i)):\n",
    "            i[j]=i[j].replace(\",\",\"\")\n",
    "            if len(i[j])>2:\n",
    "                i[j]=\"\"\n",
    "        \n",
    "        i=str(i)\n",
    "\n",
    "# Creating a list, to save the clean date.\n",
    "        date=[]\n",
    "\n",
    "# If a date is left, we find the date, and save it.        \n",
    "        if numb(i)!=[]:\n",
    "            date.append(int(numb(i)[0]))\n",
    "            \n",
    "# Then we add the found month      \n",
    "        date.append(m)\n",
    "        \n",
    "# And lastly we are adding the year.      \n",
    "        date.append(year)     \n",
    "\n",
    "# Then we are making it back to a string written as dd/mm/yyyy.\n",
    "        date = str(date)[1:len(str(date))-1]\n",
    "        date = date.replace(\"'\",\"\")\n",
    "\n",
    "# Adding the clean date, to the list of clean dates.\n",
    "        Date_clean.append(date)\n",
    "\n",
    "# If the date was NaN, then we just add the year from df[\"year\"], to the list.            \n",
    "    else:\n",
    "        Date_clean.append(year)\n",
    "\n",
    "# Replace the old date with the clean date. \n",
    "df[\"Date\"]=Date_clean\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2949969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning up the data for Deaths as they are also messy\n",
    "\n",
    "\n",
    "# We create an empty list we will use for sorting, like previous.\n",
    "l=[]\n",
    "\n",
    "for i in df[\"Deaths\"]:\n",
    "# Here we create a condition. If an entry is NAN then it is False. We are only interested in\n",
    "# rows which have info in them.    \n",
    "    if i==i:\n",
    "# Removing all [],(), comma (,) and what are written inside them.\n",
    "        i=i.lower()\n",
    "        i=re.sub(r\"\\([^()]*\\)\", \"\",i)\n",
    "        i=re.sub(r\"\\[[^()]*\\]\", \"\",i)\n",
    "        i=re.sub(\"\\,\", \"\",i)\n",
    "        \n",
    "# Checking if there is only intergers (numbers).\n",
    "        try: \n",
    "            int(i)\n",
    "# If text is found, it will separate the number and string element with our function\n",
    "        except ValueError:           \n",
    "            i_num = numb(i)\n",
    "            \n",
    "# Splitting the remaning deaths into numberlist.\n",
    "# If only one number in the list, then the number is set to number of deaths\n",
    "            if len(i_num)==1:\n",
    "                i=int(i_num[0])\n",
    "            \n",
    "# If there are two numbers, they shall be added together, under the following condition.\n",
    "# We have looked through the deaths, and this is the cases, where the number needed to be added together. \n",
    "            elif i.find(\"plus \")>-1 or i.find(\"11 1 \")>-1 or i.find(\"passengers, \")>-1 or i.find(\" and \")>-1:\n",
    "            \n",
    "                i=(int(i_num[0])+int(i_num[-1]))   \n",
    "            \n",
    "# Setting \"none\" to 0 deaths.\n",
    "            elif i==\"none\":\n",
    "                i=0\n",
    "            \n",
    "# All the remaning numbers are we taking the mean.\n",
    "            else:           \n",
    "                \n",
    "                i=(int(i_num[0])+int(i_num[-1]))/2\n",
    "            \n",
    "# Making a list with all the number of deaths         \n",
    "        l.append(int(i))\n",
    "        \n",
    "    else:\n",
    "        l.append(\"Unknows number of deaths\")\n",
    "    \n",
    "# Replacing the number of deaths with the list.\n",
    "df[\"Deaths\"]=l\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef03869e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#CREATE CLEAN CSV DATA\n",
    "# Now that we are finished with the cleaning process, we can export a csv file\n",
    "# which we nearly can use for the final visualisation process\n",
    "# We just need to add all the missing coordinats we can find, but we do that in another script.\n",
    "df.to_csv('CleanData.csv', index=False, encoding='utf-8')\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# -*- coding: utf-8 -*-",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
